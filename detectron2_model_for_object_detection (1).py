# -*- coding: utf-8 -*-
"""Detectron2 Model for Object Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CxUFEVMbXp3rXuQMSn9AjWCcNG9vxXgL

### **Ensure all dependencies are installed**
"""

!apt-get install tesseract-ocr
!pip install pytesseract
!pip install pdf2image  # Install the pdf2image module
!apt-get install poppler-utils  # Install the Poppler utilities, including pdfinfo
!pip install pyyaml==5.1
!pip install 'git+https://github.com/facebookresearch/detectron2.git'
!pip install PyMuPDF
!pip install openpyxl
!pip install pandas
!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118
!nvidia-smi # Check if you have a GPU

"""### **Import required modules and initialize Detectron2**"""

import torch
import torchvision
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision import transforms
import cv2
import pandas as pd
import pdf2image
import pytesseract
import os
from google.colab import files
from detectron2.structures import BoxMode
from detectron2.utils.logger import setup_logger
from detectron2.engine import DefaultTrainer, DefaultPredictor
from detectron2.config import get_cfg
from detectron2 import model_zoo
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog, DatasetCatalog
from detectron2.data.datasets import register_coco_instances
from detectron2.evaluation import COCOEvaluator, inference_on_dataset
from detectron2.data import build_detection_test_loader

setup_logger()

"""### **Convert PDF to images**"""

def pdf_to_images(pdf_path, output_folder):
    pdf_document = fitz.open(pdf_path)
    for page_num in range(len(pdf_document)):
        page = pdf_document.load_page(page_num)
        pix = page.get_pixmap()
        output_image_path = os.path.join(output_folder, f"page_{page_num}.png")
        pix.save(output_image_path)

"""### **Register datasets**"""

def register_datasets():
    register_coco_instances("my_dataset_train", {}, "/content/train/_annotations.coco.json", "/content/train")
    register_coco_instances("my_dataset_test", {}, "/content/test/_annotations.coco.json", "/content/test")

register_datasets()

!curl -L "https://app.roboflow.com/ds/CPov9DdVKF?key=LZLBWEaSRC" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip

"""### **Visualize dataset samples**"""

import random
import cv2  # Import the OpenCV library
import random  # Import the random module
from google.colab.patches import cv2_imshow # Import the patch for cv2.imshow in Colab

my_dataset_train_metadata = MetadataCatalog.get("my_dataset_train")
dataset_dicts = DatasetCatalog.get("my_dataset_train")

for d in random.sample(dataset_dicts, 3):  # Now you can use random.sample
    img = cv2.imread(d["file_name"])
    visualizer = Visualizer(img[:, :, ::-1], metadata=my_dataset_train_metadata, scale=0.5)
    vis = visualizer.draw_dataset_dict(d)
    cv2_imshow(vis.get_image()[:, :, ::-1]) # Use cv2_imshow instead of cv2.imshow
    cv2.waitKey(0) # Wait for a key press
    cv2.destroyAllWindows() # Close the display window

"""### **Setup Detectron2 model**"""

cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml"))
cfg.DATASETS.TRAIN = ("my_dataset_train",)
cfg.DATASETS.TEST = ("my_dataset_test",)
cfg.DATALOADER.NUM_WORKERS = 2
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml")
cfg.SOLVER.IMS_PER_BATCH = 2
cfg.SOLVER.BASE_LR = 0.0025
cfg.SOLVER.MAX_ITER = 80
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4  # column_icon, column_number, column_length, column_width
cfg.MODEL.DEVICE = "cpu"  # Explicitly set the device to CPU

os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
trainer = DefaultTrainer(cfg)
trainer.resume_or_load(resume=False)
trainer.train()

"""### **Tesseract executable path**"""

pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'

"""### **Load the trained model**"""

cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5
predictor = DefaultPredictor(cfg)

"""### **Evaluate the model**"""

evaluator = COCOEvaluator("my_dataset_test", output_dir="./output")
val_loader = build_detection_test_loader(cfg, "my_dataset_test")
print(inference_on_dataset(predictor.model, val_loader, evaluator))

from detectron2.utils.visualizer import ColorMode
dataset_dicts = DatasetCatalog.get("my_dataset_train")
for d in random.sample(dataset_dicts, 3):
    im = cv2.imread(d["file_name"])
    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format
    v = Visualizer(im[:, :, ::-1],
                  metadata=my_dataset_train_metadata,
                  scale=0.5,
                  instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models
    )
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    cv2_imshow(out.get_image()[:, :, ::-1])

"""### **Function to predict columns and extract text using Tesseract**"""

# Class mapping (Assuming these are your classes)
your_classes = {0: 'column_icon', 1: 'column_number', 2: 'column_length', 3: 'column_width'}

def predict_columns(image_path):
    img = cv2.imread(image_path)
    outputs = predictor(img)
    instances = outputs["instances"].to("cpu")
    boxes = instances.pred_boxes if instances.has("pred_boxes") else None
    scores = instances.scores if instances.has("scores") else None
    classes = instances.pred_classes if instances.has("pred_classes") else None

    detected_objects = [] # Indent this line to match the level of the 'for' loop below
    for box, score, cls in zip(boxes, scores, classes):
        if score > 0.5:
            x1, y1, x2, y2 = box.numpy()
            width = x2 - x1
            height = y2 - y1
            columns_info.append({
                "class": cls.item(),
                "x1": x1,
                "y1": y1,
                "x2": x2,
                "y2": y2,
                "width": width,
                "height": height,
            })
    return detected_objects

# Function to extract text from a bounding box using Tesseract
def extract_text_from_bbox(image_path, bbox):
    img = cv2.imread(image_path)
    x1, y1, x2, y2 = map(int, bbox)
    roi = img[y1:y2, x1:x2]
    roi_gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)  # Convert to grayscale
    roi_thresh = cv2.threshold(roi_gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]  # Apply thresholding
    text = pytesseract.image_to_string(roi_thresh)
    return text.strip()

"""### **Process the PDF and extract information**"""

def process_pdf(pdf_path):
    images = pdf2image.convert_from_path(pdf_path)
    all_detected_objects = []

    for i, image in enumerate(images):
        image_path = f'temp_image_{i}.png'
        image.save(image_path)
        detected_objects = predict_columns(image_path)
        print(f"Detected objects in page {i}: {detected_objects}")  # Debug: print detected objects
        for obj in detected_objects:
            obj['text'] = extract_text_from_bbox(image_path, obj['bbox'])
            print(f"Extracted text for {obj['label']}: {obj['text']}")  # Debug: print extracted text
        all_detected_objects.extend(detected_objects)
        os.remove(image_path)  # Clean up the temporary image file

    return all_detected_objects

"""### **Save extracted information to an Excel file**"""

def save_to_excel(detected_objects, output_path):
    data = {
        'Column Number': [],
        'Column Length': [],
        'Column Width': []
    }

    # Make sure the indentation here is consistent (either all spaces or all tabs)
    for obj in detected_objects:
        if obj['label'] == 'column_number':
            data['Column Number'].append(obj['text'])
        elif obj['label'] == 'column_length':
            data['Column Length'].append(obj['text'])
        elif obj['label'] == 'column_width':
            data['Column Width'].append(obj['text'])

    max_length = max(len(data['Column Number']), len(data['Column Length']), len(data['Column Width']))

    for key in data:
        data[key] += [None] * (max_length - len(data[key]))

    df = pd.DataFrame(data)
    df.to_excel(output_path, index=False)

!pip install detectron2 opencv-python

import detectron2
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog
import torch
import numpy as np
import cv2

def predict_columns(image_path):
    # Load the configuration and set the model path
    cfg = get_cfg()
    cfg.merge_from_file(detectron2.model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml"))
    cfg.MODEL.WEIGHTS = "/content/output/model_final.pth"  # Replace with your actual model file path

    # Set the device to CPU
    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4  # Make sure this matches the number of classes in your trained model
    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Set threshold for this model
    cfg.MODEL.DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

    predictor = DefaultPredictor(cfg)

    # Load the image as a NumPy array
    img = cv2.imread(image_path)

    # Convert the image to RGB format (if necessary)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    with torch.no_grad():
        outputs = predictor(img)  # Pass the NumPy array to the predictor

    # Extract detected objects and their information
    instances = outputs["instances"].to("cpu")
    detected_objects = []
    for i in range(len(instances)):
        class_id = instances.pred_classes[i].item()
        bbox = instances.pred_boxes[i].tensor.numpy()[0]  # Assuming you want bounding boxes
        # ... extract other relevant information like scores, etc. ...
        detected_objects.append({
            'label': class_id,  # Replace with your class label mapping
            'bbox': bbox,
            # ... other information ...
        })

    return detected_objects  # Explicitly return the detected objects

# Assuming 'process_pdf' returns the extracted column information
all_columns_info = process_pdf('/content/Centerline_test.pdf')  # Replace with your PDF file path

columns_data = []
for info in all_columns_info:
    if info["class"] == 0:  # Assuming 0 is for column_icon
        columns_data.append({
            "Column Number": "Extracted",  # Placeholder, extraction logic needed
            "Column Length": info["height"],
            "Column Width": info["width"]
        })

!pip install detectron2 opencv-python

import detectron2
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog
import torch
import numpy as np
import cv2

def predict_columns(image_path):
    # Load the configuration and set the model path
    cfg = get_cfg()
    cfg.merge_from_file(detectron2.model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml"))
    cfg.MODEL.WEIGHTS = "/content/output/model_final.pth"  # Replace with your actual model file path

    # Set the device to CPU
    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4  # Make sure this matches the number of classes in your trained model
    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Set threshold for this model
    cfg.MODEL.DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

    predictor = DefaultPredictor(cfg)

    # Load the image as a NumPy array
    img = cv2.imread(image_path)

    # Convert the image to RGB format (if necessary)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # Initialize columns_info list to store detected column information
    columns_info = []  # Initialize columns_info here

    with torch.no_grad():
        outputs = predictor(img)  # Pass the NumPy array to the predictor
        instances = outputs["instances"].to("cpu")

        # Iterate through detected instances and extract information

        # Extract detected objects and their information
        detected_objects = []
        for i in range(len(instances)):
            bbox = instances.pred_boxes[i].tensor.numpy()[0]
            x1, y1, x2, y2 = bbox
            width = x2 - x1
            height = y2 - y1
            cls = instances.pred_classes[i]  # Extract predicted class
            # The second for loop was indented one level too deep, causing the error.
            # It should be at the same level as the first for loop.
            class_id = instances.pred_classes[i].item()
            bbox = instances.pred_boxes[i].tensor.numpy()[0]  # Assuming you want bounding boxes
            # ... extract other relevant information like scores, etc. ...
            detected_objects.append({
                'label': class_id,  # Replace with your class label mapping
                'bbox': bbox,  # Make sure 'bbox' is included in the dictionary
                # ... other information ...
            }) # Make sure this closing bracket is at the correct indentation level
            columns_info.append({
                "class": cls.item(),
                "x1": x1,
                "y1": y1,
                "x2": x2,
                "y2": y2,
                "width": width,
                "height": height
            })

    return detected_objects # Return detected_objects instead of columns_info

# Upload the PDF file
uploaded = files.upload()
pdf_path = list(uploaded.keys())[0]

# Process the PDF and extract column information
detected_objects = process_pdf(pdf_path)

# Save the extracted information to Excel
output_excel_path = pdf_path.replace('.pdf', '_columns_info.xlsx')
save_to_excel(detected_objects, output_excel_path)

print(f"Columns information extracted and saved to {output_excel_path}")

# Function to predict columns in an image
def predict_columns(image_path):
    cfg = get_cfg()
    cfg.merge_from_file(detectron2.model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml"))
    cfg.MODEL.WEIGHTS = "/content/output/model_final.pth"  # Replace with your actual model file path
    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4  # Make sure this matches the number of classes in your trained model
    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Set threshold for this model
    cfg.MODEL.DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

    predictor = DefaultPredictor(cfg)

    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    with torch.no_grad():
        outputs = predictor(img)

    instances = outputs["instances"].to("cpu")
    detected_objects = []
    for i in range(len(instances)):
        class_id = instances.pred_classes[i].item()
        bbox = instances.pred_boxes[i].tensor.numpy()[0]
        detected_objects.append({
            'label': your_classes[class_id],
            'bbox': bbox,
            'score': instances.scores[i].item()
        })
        print(f"Detected {your_classes[class_id]} with confidence {instances.scores[i].item()} at {bbox}")

    return detected_objects

# Function to extract text from a bounding box using Tesseract
def extract_text_from_bbox(image_path, bbox):
    img = cv2.imread(image_path)
    x1, y1, x2, y2 = map(int, bbox)
    roi = img[y1:y2, x1:x2]
    roi_gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
    roi_thresh = cv2.threshold(roi_gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
    text = pytesseract.image_to_string(roi_thresh)
    print(f"Extracted text: {text.strip()} from bounding box: {bbox}")
    return text.strip()

# Function to process the PDF and extract column information
def process_pdf(pdf_path):
    images = pdf2image.convert_from_path(pdf_path)
    all_detected_objects = []

    for i, image in enumerate(images):
        image_path = f'temp_image_{i}.png'
        image.save(image_path)
        detected_objects = predict_columns(image_path)
        print(f"Detected objects in page {i}: {detected_objects}")
        for obj in detected_objects:
            obj['text'] = extract_text_from_bbox(image_path, obj['bbox'])
            print(f"Extracted text for {obj['label']}: {obj['text']}")
        all_detected_objects.extend(detected_objects)
        os.remove(image_path)

    return all_detected_objects

!pip install pdf2image detectron2 opencv-python

import cv2
import torch
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.data import MetadataCatalog
from detectron2.model_zoo import model_zoo
from pdf2image import convert_from_path

# Load Detectron2 model configuration and weights
cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml"))
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml")
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Set a threshold for predictions

predictor = DefaultPredictor(cfg)

# Path to your PDF file
pdf_path = '/content/Centerline_test.pdf'  # Replace with your PDF file path

# Convert PDF to images
images = convert_from_path(pdf_path)

# Process each image
for image in images:
    # Convert PIL Image to OpenCV format
    open_cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)

    # Make predictions on the image
    outputs = predictor(open_cv_image)

    # Get predicted classes and bounding boxes
    predicted_classes = outputs["instances"].pred_classes
    predicted_boxes = outputs["instances"].pred_boxes

    # Identify unique predicted class indices
    unique_classes = torch.unique(predicted_classes).tolist()

    # Map class indices to class names (handling potential unknown classes)
    your_classes = {0: 'column_icon', 1: 'column_number', 2: 'column_length', 3: 'column_width'}
    predicted_class_names = [your_classes.get(cls.item(), 'unknown') for cls in predicted_classes] # Use .get() to handle missing keys

    # Print the results for the current image
    print("Predictions for this page:")
    for cls_name, box in zip(predicted_class_names, predicted_boxes):
        print(f"Class: {cls_name}, Box: {box}")
    print("-" * 30)  # Separator between pages

# Print unique predicted classes for inspection
print("Unique predicted classes:", unique_classes)

# Function to save extracted information to an Excel file
def save_to_excel(detected_objects, output_path):
    data = {
        'Column Number': [],
        'Column Length': [],
        'Column Width': []
    }

    for obj in detected_objects:
        if obj['label'] == 'column_number':
            data['Column Number'].append(obj['text'])
        elif obj['label'] == 'column_length':
            data['Column Length'].append(obj['text'])
        elif obj['label'] == 'column_width':
            data['Column Width'].append(obj['text'])

    max_length = max(len(data['Column Number']), len(data['Column Length']), len(data['Column Width']))

    for key in data:
        data[key] += [None] * (max_length - len(data[key]))

    df = pd.DataFrame(data)
    df.to_excel(output_path, index=False)
    print(f"Data saved to {output_path}")

# Upload the PDF file
uploaded = files.upload()
pdf_path = list(uploaded.keys())[0]

# Process the PDF and extract column information
detected_objects = process_pdf(pdf_path)

# Save the extracted information to Excel
output_excel_path = pdf_path.replace('.pdf', '_columns_info.xlsx')
save_to_excel(detected_objects, output_excel_path)

print(f"Columns information extracted and saved to {output_excel_path}")